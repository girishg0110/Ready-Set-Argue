# -*- coding: utf-8 -*-
"""Ready, Set, Argue!

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s_8pEQ8SgIxKPpyVyHKfBuYGZylDKdCg
"""

#!pip install convokit
#!pip install nltk
#!pip install tqdm
from convokit import Corpus, download
import pandas as pd
import nltk 
from nltk.tokenize import sent_tokenize, word_tokenize
from tqdm import tqdm
import os
#nltk.download('punkt')

"""## Preprocessing"""

corpus = Corpus(filename=download("iq2-corpus"))
corpus.print_summary_stats()

debate_df = corpus.get_conversations_dataframe()
# Cast indices to integer
debate_df.index = debate_df.index.astype('int64')

debate_df.head()

transcript_df = corpus.get_utterances_dataframe()
transcript_df.head()

# Remove unused columns
del transcript_df['timestamp'], transcript_df['speaker'], transcript_df['reply_to'], \
    transcript_df['meta.nontext'], transcript_df['meta.segment'], transcript_df['meta.paragraphbreaks']
# Remove non-debaters (i.e. moderators) and change representation to true/false
transcript_df = transcript_df[transcript_df['meta.speakertype'] != 'mod'][transcript_df['meta.speakertype'] != 'host']
transcript_df['speaker_is_pro'] = transcript_df.apply(lambda r: r['meta.speakertype'] == 'for', axis=1)
del transcript_df['meta.speakertype']

# Cast all columns to appropriate types
transcript_df['conversation_id'] = transcript_df['conversation_id'].astype('int64')
transcript_df['speaker_is_pro'] = transcript_df['speaker_is_pro'].astype('bool')
transcript_df.index = transcript_df.index.astype('int64')

transcript_df.head()

# Remove tied debates from dataframe
old_size = transcript_df.shape[0]
no_tie_filter = debate_df.loc[transcript_df['conversation_id'], 'meta.winner'] != 'tie'
no_tie_filter.index = transcript_df.index
transcript_df = transcript_df[no_tie_filter]
print("Reduced size from {0} examples to {1} examples.".format(
    old_size,
    transcript_df.shape[0]
))

transcript_df['pro_won'] = transcript_df.apply(
    lambda r : debate_df.loc[r['conversation_id'], 'meta.winner'] == 'for',
    axis = 1
)
transcript_df.head()

min_tokens_in_sentence = 10
sentence_df = pd.DataFrame()
counter = 0
total_count = 0
for index, row in transcript_df.iterrows():
  for tokenized_sentence in sent_tokenize(row['text']):
    if (len(word_tokenize(tokenized_sentence)) >= min_tokens_in_sentence):
      sentence_df = sentence_df.append({
      'sentence': tokenized_sentence, 
      'conversation_id' : row['conversation_id'], 
      'speaker_is_pro' : row['speaker_is_pro'],
      'pro_won' : row['pro_won']
      }, ignore_index = True)
  counter += 1
  if (counter == 1000): 
    total_count += 1
    counter = 0
    print("{0} entries converted.".format(total_count * 1000))
print("All {0} entries converted. {1} sentences in dataset.".format(transcript_df.shape[0],
                                                                    sentence_df.shape[0]))

# Cast each column to appropriate type
sentence_df['conversation_id'] = sentence_df['conversation_id'].astype('int64')
sentence_df['pro_won'] = sentence_df['pro_won'].astype('bool')
sentence_df['speaker_is_pro'] = sentence_df['speaker_is_pro'].astype('bool')

FILE_PATH = "./drive/MyDrive/RSA!/"
sentence_df.to_csv(FILE_PATH + 'iq2-sents.csv')
print("{0} sentences to be classified.".format(sentence_df.shape[0]))

sentence_df.head()

"""## Feature Extraction"""

from nltk.sentiment.vader import SentimentIntensityAnalyzer
import numpy as np
import re
#nltk.download('vader_lexicon')

"""<h4> Sentiment Analysis </h4>"""

features = pd.read_csv(FILE_PATH + 'iq2-sents.csv')
s = SentimentIntensityAnalyzer()
all_senti_scores = { row['sentence'] : s.polarity_scores(row['sentence']) for index, row in features.iterrows()}

features['senti_score'] = features['sentence'].apply(lambda x : all_senti_scores[x]['compound'])
features['senti_class_neg'] = features['sentence'].apply(
    lambda x : all_senti_scores[x]['neg'] > all_senti_scores[x]['pos'] and
                all_senti_scores[x]['neg'] > all_senti_scores[x]['neu']
)
features['senti_class_pos'] = features['sentence'].apply(
    lambda x : all_senti_scores[x]['pos'] > all_senti_scores[x]['neg'] and
                all_senti_scores[x]['pos'] > all_senti_scores[x]['neu']
)
features['senti_class_neu'] = features.apply(
    lambda x : (not x['senti_class_pos']) and (not x['senti_class_neg']), 
    axis = 1
)
features.head()

"""<h4> Regex Tonal Features </h4>"""

regex_features = {
    'confusion1': r"(ˆ| )i (\S + ){,2}(not|n’t|never) (understand|know)",
    'confusion2': r"(not|n’t) make sense",
    'confusion3': r"(ˆ| )i (\S + ){,2}(curious|confused)",
    'confusion4' : r"(ˆ| )i (\S + ){,2}wonder",
    'confusion5' : r"(me|myself) wonder",
    'why_how' : r"(ˆ| )(why|how).*\?",
    'question_other' : r"[?]",
    'prediction': r"(am$|$’m$|$are$|$’re$|$is$|$’s) (not )?(going to$|$gonna)",
    'hypothetical' : r"(ˆ|, )if|unless",
    'citation': r"(said|reported|mentioned|declared|claimed|admitted|explained|insisted|promised|suggested|recommended|denied|blamed|apologized|agreed|answered|argued|complained|confirmed|proposed|replied|stated|told|warned|revealed|according to|) that [ˆ.,!?]",
    'comparison': "(than|compared to)",
    'examples': r"(ˆ| )(for example|for instance|such as|e\.g\.)( |$)",
    'definition' : r"(define|definition)",
    'personal_story' : r"(think|believe|see|know|feel|say|understand|mean|sure|agree|argue|consider|guess|realize|hope|support|aware|disagree|post|mention|admit|accept|assume|convince|wish|appreciate|speak|suppose|doubt|explain|wonder|discuss|view|suggest|recognize|respond|acknowledge|clarify|state|sorry|advocate|propose|define|apologize|curious|figure|claim|concede|debate|list|oppose|describe|suspect|reply|bet|realise|defend|convinced|offend|concern|intend|certain|conclude|reject|challenge|thank|con-done|value|skeptical|contend|anticipate|maintain|justify|recommend|confident|promise|guarantee|comment|unsure|elaborate|posit|swear|dispute|imply|misunderstand)",
    'you' : r"(you|your|yours)", 
    'we' : r"(ˆ| )we |(?<!the) (us|our|ours)( |$)"
}
non_binary_regex = {
    'hedge_pos' : r"(allegedly|apparently|appear to|conceivably|could be|doubtful|fairly|hopefully|i assume|i believe|i do not believe|i doubt|i feel|i do not feel|i guess|ispeculate|i think|i do not think|if anything|imo|imply|in my mind|in my opinion|in myunderstanding|in my view|it be possible|it look like|it do not look like|kind of|mainly|may|maybe|might|my impression be|my thinking be|my understanding be|perhaps|possibly|potentially|presumably|probably|quite|rather|relatively|seem|somehow|somewhat|sort of|supposedly|to my knowledge|virtually|would)",
    'hedge_neg' : r"(be definite|definitely|directly|enormously|entirely|evidently|exactly|explicitly|extremely|fundamentally|greatly|highly|in fact|incredibly|indeed|inevitably|intrinsically|invariably|literally|necessarily|no way|be obvious|obviously|perfectly|precisely|really|be self-evident|be sure|surely|totally|truly|be unambiguous|unambiguously|be undeniable|undeniably|undoubtedly|beunquestionable|unquestionably|very|wholly)",
    'qualification_pos' : r"(a bit|a few|a large amount of|a little|a lot of|a number of|almost|approximately|except|generally|if|in general|largely|likely|lots of|majority of|many|more or less|most|mostly|much|nearly|normally|occasionally|often|overall|partly|plentyof|rarely|roughly|several|some|sometimes|tend|ton of|tons of|typically|unless|unlikely|usually)",
    'qualification_neg' : r"(all|always|every|everybody|everyone|everything|never|no|no one|nobody|none|neither|not any|ever|forever)"
}

for feat_exp in regex_features:
  features[feat_exp] = features['sentence'].apply(lambda x : len(re.findall(regex_features[feat_exp], x)) > 0)
  print("Feature {0} extracted.".format(feat_exp))

for feat_exp in non_binary_regex:
  features[feat_exp] = features['sentence'].apply(lambda x : len(re.findall(non_binary_regex[feat_exp], x)))
  print("Feature {0} extracted.".format(feat_exp))

features['hedge'] = features[['hedge_pos', 'hedge_neg']].apply(
    lambda x : x['hedge_pos'] - x['hedge_neg'],
    axis = 1
)
features['qualification'] = features[['qualification_pos', 'qualification_neg']].apply(
    lambda x : x['qualification_pos'] - x['qualification_neg'],
    axis = 1
)
del features['hedge_pos']
del features['hedge_neg']
del features['qualification_pos']
del features['qualification_neg']
features[['hedge', 'qualification']]

features['confusion'] = features.apply(
    lambda row : row['confusion1'] or row['confusion2'] or row['confusion3'] or row['confusion4'] or row['confusion5'], 
    axis=1
)
del features['confusion1']
del features['confusion2']
del features['confusion3']
del features['confusion4']
del features['confusion5']

"""<h4> Lexicon-Based Features </h4>"""

features['tokens'] = features['sentence'].apply(lambda x: word_tokenize(x))

AD_df = pd.read_csv(FILE_PATH + 'arousal_dominance_lexicon.csv").loc[:, ['Word', 'A.Mean.Sum', 'D.Mean.Sum']]
AD_df.index = AD_df['Word']
del AD_df['Word']
AD_df

arousal_df = AD_df.loc[:, 'A.Mean.Sum']
dominance_df = AD_df.loc[:, 'D.Mean.Sum']

concrete_df = pd.read_csv(FILE_PATH + 'concreteness_lexicon.csv').loc[:, ['Word', 'Conc.M']]
concrete_df.index = concrete_df['Word']
del concrete_df['Word']
concrete_df

subjectivity_dict = {}
with open(FILE_PATH + 'subjectivity_lexicon') as file:
  for i, line in enumerate(file.readlines()):
    word = re.findall('word1=(.*?) pos1=', line)[0]
    polarity = -1 if re.findall('priorpolarity=(.*?)\n', line)[0] == 'negative' else 1
    strength = 0.5 if re.findall('type=(.*?) len=', line)[0] == 'weaksubj' else 1
    subjectivity_dict[word] = polarity * strength
subjective_df = pd.DataFrame(subjectivity_dict.values(), index = subjectivity_dict.keys())
subjective_df['Subjectivity'] = subjective_df[0]
del subjective_df[0]
subjective_df

def average_df_value_on_tokenized_sentence(tokens, df):
  count = 0
  total_score = 0.0
  for word in tokens:
    try:
      total_score += df[word]
      count += 1
    except KeyError:
      pass
  return total_score/count if count != 0 else 0.0

for feat, df in zip(["arousal", "dominance", "concreteness", "subjectivity"], [arousal_df, dominance_df, concrete_df, subjective_df]):
  features[feat] = features['tokens'].apply(lambda x : average_df_value_on_tokenized_sentence(x, df))
  print("Feature {0} extracted.".format(feat))

"""<h4> Kialo Knowledge Base Features </h4>"""

kialo_df = pd.read_csv(FILE_PATH + "kialo.csv")
kialo_df['tokens'] = kialo_df['text'].apply(lambda x : word_tokenize(x))
kialo_df.head()

kialo_df['unique_tokens'] = kialo_df['tokens'].apply(lambda x : set(x))
features['matched_kialo_statements'] = features['tokens'].apply(
    lambda s : list(abridged_kialo[abridged_kialo['unique_tokens'].apply(
        lambda r : len(set(s).intersection(r)) >= 5
    )].index)
)
print("{0} matches found in knowledge base.".format(
    features['matched_kialo_statements'].map(
        lambda x : len(x)
      ).sum()
    )
)

kialo_df['rel-pro'] = kialo_df['rel-pro'].astype('int64')
kialo_df['rel-neu'] = kialo_df['rel-neu'].astype('int64')
kialo_df['rel-con'] = kialo_df['rel-con'].astype('int64')

# Frequency
features['kialo_frequency'] = features['matched_kialo_statements'].apply(
    lambda x : np.log(len(x) + 1)/np.log(2)
)

# Attractiveness
features['kialo_avg_num_responses'] = features['matched_kialo_statements'].apply(
    lambda x : kialo_df.loc[x].apply(
        lambda c : c['rel-neu'] + c['rel-pro'] + c['rel-con'],
        axis = 1
    ).sum()/len(x) if len(x) > 0 else 0.0
)
features['kialo_attractiveness'] = features['kialo_avg_num_responses'].apply(
    lambda x : np.log(x + 1) / np.log(2)
)

# Extremeness
def safe_div(a, b):
  return 0 if b == 0 else a * 1.0 / b

kialo_df['pro_proportion'] = kialo_df.apply(
    lambda x : safe_div(x['rel-pro'], x['rel-pro'] + x['rel-neu'] + x['rel-con']), 
    axis = 1
)
kialo_df['neg_proportion'] = kialo_df.apply(
    lambda x : safe_div(x['rel-con'], x['rel-pro'] + x['rel-neu'] + x['rel-con']),
    axis = 1
)
features['kialo_extremeness'] = features['matched_kialo_statements'].apply(
    lambda x : kialo_df.loc[x].apply(
        lambda c : abs(c['pro_proportion'] - c['neg_proportion']), 
        axis = 1
    ).sum()/len(x) if len(x) > 0 else 0.0
)

del features['sentence']
del features['tokens']
del features['matched_kialo_statements']
features.head()

"""<h1> Predicting IQ2 Debate Results <h1>"""

from sklearn.linear_model import SGDClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from itertools import product

aggregate_df = pd.DataFrame()
for i in sorted(set(features['conversation_id'])):
  conv_speeches = features[features['conversation_id'] == i]
  pro_details = conv_speeches[conv_speeches['speaker_is_pro']]
  con_details = conv_speeches[~conv_speeches['speaker_is_pro']]
  debate_details = {
      'conversation_id' : i, 
      'pro_won' : conv_speeches['pro_won'].all(),
  }
  for side, performance in zip(["+", "-"], [pro_details, con_details]):
    for col in features.columns:
      if (col != 'conversation_id' and col != 'pro_won' and col != 'speaker_is_pro'):
        debate_details[side + col] = performance[col].sum()
  aggregate_df = aggregate_df.append(debate_details, ignore_index = True)
print("There are {0} rows in this DataFrame, the same as the number of unique debates".format(aggregate_df.shape[0]))
aggregate_df.head()

X_df = aggregate_df.loc[:, aggregate_df.columns != 'pro_won']
X_df = X_df.loc[:, X_df.columns != 'conversation_id']
y_df = aggregate_df['pro_won']
X = X_df.to_numpy()
y = y_df.to_numpy()

training_examples = int(0.8 * X.shape[0])
X_train = X[:training_examples]
y_train = y[:training_examples]
X_test = X[training_examples:]
y_test = y[training_examples:]
print("{0} training examples each with {1} features. {2} testing examples.".format(X_train.shape[0], X_train.shape[1], X_test.shape[0]))

reg_strengths = [1e-1, 1e-2, 1e-3, 1e-4]
penalties = ['l1', 'l2']
losses = ['hinge', 'log']
early_stopping_options = [True, False]

def train(X, y, alpha = 0.1, loss = 'log', penalty = 'l2', early_stopping=False):
  classifier = make_pipeline(
      StandardScaler(),
      SGDClassifier(loss=loss, penalty=penalty, alpha=alpha, early_stopping=early_stopping)
  )
  classifier.fit(X, y)
  return classifier

best_classifier = None
best_test_acc = 0.0
best_settings = {}
all_results = []
for alpha, penalty, loss, early_stopping in product(reg_strengths, penalties, losses, early_stopping_options):
  print("Regularization strength = {0}. Penalty = {1}. Loss = {2}. Early stopping {3}.".format(alpha, penalty, loss, "enabled" if early_stopping else "disabled"))
  classifier = train(X_train, y_train, alpha=alpha, penalty=penalty, loss=loss, early_stopping=early_stopping)
  train_acc = classifier.score(X_train, y_train)
  print("\tTraining accuracy = " + str(train_acc*100) + "%")
  test_acc = classifier.score(X_test, y_test)
  print("\tTesting accuracy = " + str(test_acc*100) + "%")

  all_results.append({
      'alpha' : alpha,
      'penalty' : penalty,
      'loss' : loss,
      'early_stopping' : early_stopping,
      'final_train_acc' : train_acc,
      'final_test_acc' : test_acc,
      'classifier' : classifier
  })

  if (best_test_acc < test_acc):
    best_classifier = classifier
    best_test_acc = test_acc
    best_settings['alpha'] = alpha
    best_settings['penalty'] = penalty
    best_settings['loss'] = loss
    best_settings['early_stopping'] = early_stopping
print(
    "\nThe best testing accuracy was {0}, acheived by training by SGD on {1} loss with {2} regularization and regularization strength set to {3} {4} early stopping.".format(
        best_test_acc, best_settings['loss'], best_settings['penalty'], best_settings['alpha'], 'with' if best_settings['early_stopping'] else 'without'
    )
)